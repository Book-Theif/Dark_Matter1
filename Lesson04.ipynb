{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 04: \n",
    "### Dylan Temples \n",
    "July 2, 2020\n",
    "\n",
    "The goal of this lesson is to provide an introduction to Monte-Carlo methods, and how to run simulations in Python.\n",
    "\n",
    "## Learning Objectives\n",
    "Physics Topics:\n",
    "- Monte-Carlo Methods\n",
    "\n",
    "Programming Topics:\n",
    "- Random number generation\n",
    "- Sampling from distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo methods are an extremely powerful and useful tool in physics and other areas of science. In short, Monte-Carlo methods use random variables to predict the outcome of some scenario. Generally, any simulation you run uses Monte-Carlo Methods. In this lesson, I will demonstrate how to use MC methods to run simulations and statistically analyze the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, first let's simulate the two-step decay of $^{83m}$Kr. Imagine you have a detector that starts a timer when it detects the first 32 keV decay, and stops the time when it detects the subsequent 9 keV decay. For a single obsevation of the two-step decay, the detector tells you $t_\\Delta$, the time between the decays. The value of $t_\\Delta$ is a random variable, drawn from an exponential distribution characterized by the lifetime of the intermediate 9 keV state. First, let's look at this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr83m_9keV_halflife = 156.94 ## nanoseconds\n",
    "kr83m_9keV_tau      = kr83m_9keV_halflife / np.log(2.0)   ## still nanoseconds\n",
    "\n",
    "## Define the distribution\n",
    "def exponential_pdf(t, tau):\n",
    "    return (1./tau) * np.exp(-t/tau)\n",
    "\n",
    "## Create an array of time values to sample the distribution\n",
    "t_delta_vals = np.linspace(start=0.0 , stop=1000.0 , num=100)\n",
    "\n",
    "## Determine the value of the pdf at each time\n",
    "pdf_vals = exponential_pdf(t_delta_vals, kr83m_9keV_tau)\n",
    "\n",
    "## Plot it\n",
    "plt.figure()\n",
    "plt.plot(t_delta_vals, pdf_vals)\n",
    "plt.xlabel(r\"$t_\\Delta$ [nanoseconds]\")\n",
    "plt.ylabel(r\"PDF [ns$^{-1}$]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the probability of the 9 keV level decaying a specified time after the 32 keV level. So now let's run a simulation of the experiment. To do so we need to draw some number of events from this distribution. There are many ways to do this, but since the exponential distribution has a simple analytical form, the easiest will be to invert the cumulative distribution function.\n",
    "\n",
    "The CDF is the cumulative probability at a given value of the x-axis. For example, the CDF of the distribution above at $t_\\Delta = 200$ ns is the area under the curve from $t_\\Delta = 0$ to $t_\\Delta = 200$. The CDF of an exponential distribution is\n",
    "$$ F(t, \\tau) = \\int_0^\\infty f(t, \\tau) dt = 1 - e^{-t/\\tau} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the CDF\n",
    "def exponential_cdf(t, tau):\n",
    "    return 1.0 - np.exp(-t/tau)\n",
    "\n",
    "## Determine the value of the cdf at each time\n",
    "cdf_vals = exponential_cdf(t_delta_vals, kr83m_9keV_tau)\n",
    "\n",
    "## Plot it\n",
    "plt.figure()\n",
    "plt.plot(t_delta_vals, cdf_vals)\n",
    "plt.xlabel(r\"$t_\\Delta$ [nanoseconds]\")\n",
    "plt.ylabel(r\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "What is the probability of observing a Kr83m decay with a time separation longer than 100 ns, but shorter than 400 ns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your solution to problem 1 here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CDF has a domain that spans all prossible values of $t_\\Delta$, and its range is $\\in [0,1)$. Therefore the inverse CDF has a domain of $[0,1)$ and a range of all possible values of $t_\\Delta$. Generating random numbers from a uniform distribution between 0 and 1 is the simplest way to get a random value in most languages. Inverting the CDF allows one to sample random variables from the specified PDF, because if you feed the inverse CDF random values uniformly distributed along its range, the random variables it outputs will be distributed like the pdf. Let's test this.\n",
    "\n",
    "The following bit of code will run one simulated experiment from the detector we described above. Here we're simulating exactly 1000 decays observed by the detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the inverse CDF\n",
    "def exponential_inverse_cdf(F, tau):\n",
    "    return -1. * tau * np.log(1.-F)\n",
    "\n",
    "## generate 1000 random values uniformly distributed between 0 and 1\n",
    "F_vals = np.random.rand(1000)\n",
    "\n",
    "## calculate the inverse CDF of these values\n",
    "t_vals = exponential_inverse_cdf(F_vals, kr83m_9keV_tau)\n",
    "\n",
    "## create a histogram of these values\n",
    "t_bins = np.linspace(start=0.0 , stop=1.0e3 , num=100)\n",
    "plt.figure()\n",
    "n_entries, bin_edges, patches = plt.hist(t_vals, bins=t_bins)\n",
    "plt.xlabel(r\"$t_\\Delta$ [nanoseconds]\")\n",
    "plt.ylabel(r\"Counts per bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram looks like we expect, but how close to the PDF is it. To determine this, we'll want to plot the PDF on top of this histogram. However, since the PDF is normalized to have integral 1, we'll want to normalize the histogram in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a histogram of these values\n",
    "t_bins = np.linspace(start=0.0 , stop=1.0e3 , num=100)\n",
    "plt.figure()\n",
    "n_entries, bin_edges, patches = plt.hist(t_vals, bins=t_bins, density=1.0, label=\"Data\")\n",
    "plt.plot(t_delta_vals, pdf_vals, label=r\"Fixed PDF, $\\tau=$\"+str(kr83m_9keV_tau)+\" ns\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(r\"$t_\\Delta$ [nanoseconds]\")\n",
    "plt.ylabel(r\"PDF [ns$^{-1}$]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data from our simulated experiment is distributed as we'd expect. But if we didn't know the true distribution (i.e., the true lifetime), we would have to extract it from our data. Let's do that in the same way we extracted the electron lifetime in Lesson 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "## We extracted the bin contents and edges from the histogram above\n",
    "bin_entries = n_entries   ## note this is density, not counts\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "popt, pcov = curve_fit( f = exponential_pdf, \n",
    "                        xdata = bin_centers,\n",
    "                        ydata = bin_entries,\n",
    "                        p0=[225])\n",
    "print(\"Lifetime of 9 keV state from fit: \", popt[0], \"nanoseconds\")\n",
    "fit_pdf_values = exponential_pdf(t_delta_vals, popt[0])\n",
    "\n",
    "## Draw the normalized histogram again\n",
    "plt.figure()\n",
    "n_entries, bin_edges, patches = plt.hist(t_vals, bins=t_bins, density=1.0)\n",
    "plt.plot(t_delta_vals, pdf_vals, label=r\"PDF $\\tau=$\"+str(kr83m_9keV_tau)+\" ns\")\n",
    "plt.plot(t_delta_vals, fit_pdf_values, label=r\"Best Fit, $\\tau=$\"+str(popt[0])+\" ns\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we didn't get the exact lifetime out. But is the \"true\" value within the statistical uncertainty of our data? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_variance = pcov[0]\n",
    "tau_err = np.sqrt(tau_variance)\n",
    "\n",
    "lo_lim = popt[0] - tau_err[0]\n",
    "hi_lim = popt[0] + tau_err[0]\n",
    "\n",
    "print(\"Confidence band: \",lo_lim,\"to\",hi_lim,\"nanoseconds\")\n",
    "print(\"Result: \",popt[0],\"+/-\",tau_err[0],\"nanoseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen that we can get a reasonable lifetime result from our simulated experiment, let's see what happens if we were to do this experiment 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a function to perform a single experiment\n",
    "def RunExperiment():\n",
    "    N_obs_per_experiment = 1000\n",
    "    \n",
    "    ## generate 1000 random values uniformly distributed between 0 and 1\n",
    "    F_vals = np.random.rand(N_obs_per_experiment)\n",
    "    \n",
    "    ## sample from the PDF of Kr83m 9keV lifetime\n",
    "    t_vals = exponential_inverse_cdf(F_vals, kr83m_9keV_tau)\n",
    "    \n",
    "    ## create the bins for the histogram \n",
    "    t_bins = np.linspace(start=0.0 , stop=1.0e3 , num=100)\n",
    "\n",
    "    ## use numpy.histogram instead of matplotlib.pyplot.hist to do histogramming without making a plot\n",
    "    n_entries, bin_edges = np.histogram(t_vals, bins=t_bins, density=1.0)\n",
    "    \n",
    "    ## calculate the bin entries and centers\n",
    "    bin_entries = n_entries   ## note this is density, not counts\n",
    "    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "    \n",
    "    ## do the curve fit\n",
    "    popt, pcov = curve_fit( f = exponential_pdf, \n",
    "                        xdata = bin_centers,\n",
    "                        ydata = bin_entries,\n",
    "                        p0=[225])\n",
    "    \n",
    "    ## return the best fit value and statistical uncertainty\n",
    "    return popt[0] , np.sqrt(pcov[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the number of experiments to run\n",
    "N_experiments_to_run = 10000\n",
    "\n",
    "## create containers for the output of the experiments\n",
    "experiment_result_vals = np.zeros(N_experiments_to_run)\n",
    "experiment_result_errs = np.zeros(N_experiments_to_run)\n",
    "\n",
    "## run the experiments and save the results\n",
    "for i in np.arange(N_experiments_to_run):\n",
    "    val, err = RunExperiment()\n",
    "    experiment_result_vals[i] = val\n",
    "    experiment_result_errs[i] = err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the results of 100 experiments, let's look at the distribution of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "n_t_bins = 100\n",
    "n_entries_gauss, bin_edges_gauss, patches = plt.hist(experiment_result_vals, bins=n_t_bins, density=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the results of this experiment are normally distributed around some central value. Let's fit this to a properly normalzied gaussian to see where the central value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_pdf(x,mu,sig):\n",
    "    fac = 1./ (sig*np.sqrt(2.0*np.pi))\n",
    "    z = (x-mu)/sig\n",
    "    return fac * np.exp(-0.5*z*z)\n",
    "\n",
    "## calculate the bin entries and centers\n",
    "bin_entries = n_entries_gauss   ## note this is density, not counts\n",
    "bin_centers = 0.5 * (bin_edges_gauss[1:] + bin_edges_gauss[:-1])\n",
    "    \n",
    "## do the curve fit\n",
    "popt, pcov = curve_fit( f = gauss_pdf, \n",
    "                    xdata = bin_centers,\n",
    "                    ydata = bin_entries,\n",
    "                    p0=[225,20])\n",
    "\n",
    "## remake the figure and plot the fit\n",
    "plt.figure()\n",
    "ax1 = plt.gca()\n",
    "n_entries_gauss, bin_edges_gauss, patches = plt.hist(experiment_result_vals, density=1.0)\n",
    "\n",
    "xlims = ax1.get_xlim()\n",
    "x_fit_vals = np.linspace(start=xlims[0], stop=xlims[1], num=1000)\n",
    "y_fit_vals = gauss_pdf(x_fit_vals, popt[0], popt[1])\n",
    "ax1.set_xlim(xlims)\n",
    "\n",
    "plt.plot(x_fit_vals,y_fit_vals, label=\"Gaussian Fit, $\\mu=$\"+str(popt[0])+\" $\\pm$ \"+str(np.sqrt(pcov[0,0])))\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(r\"Fit $\\langle t_\\Delta \\rangle$ [nanoseconds]\")\n",
    "plt.ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean from the Gaussian fit should be a much closer approximation to the true value and with smaller errorbars. This is the power of repeated measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "The above illustrated how to sample from a PDF that has an easily invertable CDF. This is not always the case. Another method that only requires a numerical PDF (as opposed to analytic, though analytic also works) is known as the accept-reject method. The concept for this method is pretty straightforward.\n",
    "\n",
    "The procedure is as follows:\n",
    "1. Determine the domain and range of your PDF\n",
    "2. Generate a pair of random numbers, both from different uniform distributions. One with a value within the PDF's domain, and one with a value within the PDF's range.\n",
    "3. The random number drawn from the uniform distribution spanning the PDF's domain is your \"$x$\" value, while the other is your \"$y$\" value. If $y < PDF(x)$, then the point you've generated is below your PDF and is to be accepted. If that is not the case, draw a new pair and repeat.\n",
    "4. Keep track of the $x$ random variables\n",
    "5. Do this some number of times to get the number of samples you want. The $x$ values will be distributed according to your PDF.\n",
    "\n",
    "You should try to repeat the above experiments using the accpetion-rejection method instead of inverting the CDF.\n",
    "\n",
    "What are the drawbacks to using this method? Are there any advantages this method has over inverting the CDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your solution to Problem 2 here\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Consider a twenty-sided die (\"d20\"). For a single trial, I roll the d20 twice, if either result is above 13, I win. If they're both equal to or below 13, I lose. What is the probability that I win?\n",
    "\n",
    "This problem can be done both analytically and statistically, using the methods described above. Try it both ways, do your answers agree?\n",
    "\n",
    "Now consider two ten-sided dice (\"2d10\"), where I add the resulting numbers, instead of a d20. For a single trial I roll 2d10 twice, if either result is above 13, I win. If they're both below 13, I lose. What is the probability that I win? \n",
    "\n",
    "For example my first 2d10 roll gives me a 3 and a 5 (sum 8), my second 2d10 roll is a 9 and a 6 (sum 15), so I win. (A single trial consists of 4 dice rolls.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put your solution to Problem 3 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will do it statistically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now do it with 2d10 instead of a d20\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
